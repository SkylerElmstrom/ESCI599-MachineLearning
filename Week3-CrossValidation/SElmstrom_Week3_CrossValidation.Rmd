---
title: "Module 3 - Cross Validation"
author: "Skyler Elmstrom"
date: "1/17/2021"
output:
  html_document:
    code_folding: hide
    code_download: true
    keep_md: true
    tidy: true
---

<style>
p.caption {
  font-size: 0.9em;
  font-style: italic;
  text-align: justify;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message=F, warning=F)
```

```{r *Black points are observed penguin body mass. Blue line represents the predicted body mass from linear model.}
# Libraries
library(caret)
library(PNWColors)
library(palmerpenguins)
library(tidyverse)
```

```{r eval = F, include = F}
# Class Examples
dat <- read.csv("Data/dat.csv")

###########################
### Split/Train Methods ###
###########################

p1 <- ggplot(dat,aes(x=x,y=y)) + geom_point(size=3)
p1

lm1 <- lm(y~x,data=dat)
summary(lm1)

p1 + geom_smooth(method="lm")

# Train with 2/3 data, Test with remaining 1/3
n <- nrow(dat)
rows2test <- sample(1:n,size = n * 1/3)
dat$Test <- FALSE
dat$Test[rows2test] <- TRUE
head(dat)

table(dat$Test)

anem7 <- pnw_palette(name="Anemone",n=7,type="discrete")
p1 <- ggplot(dat,aes(x=x,y=y,color=Test))+ geom_point(size=3) +
  scale_color_manual(values=c(anem7[1],anem7[7]))
p1

testing <- dat[dat$Test,]
training <- dat[-dat$Test,]

lm1 <- lm(y~x,data=training)
summary(lm1)

p1 + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2],
                       color = anem7[1])

testing$yhat <- predict(lm1,newdata=testing)
ggplot(testing,aes(x=y,y=yhat)) + geom_point(size=3,color=anem7[7]) + 
  geom_abline(intercept = 0, slope = 1) +
  labs(main="Testing Data",x="Observed", y="Predicted")

rsq <- cor(testing$yhat,testing$y)^2
mae <- mean(abs(testing$yhat - testing$y))
rmse <- sqrt(mean((testing$yhat - testing$y)^2))
rsq

######################
### K Fold Methods ###
######################

# Create a new data.frame, drop the test column we added above,
# then randomly shuffle the data to remove any systematic ordering 
# of the rows.
dat2 <- dat[,1:2]
dat2 <- dat2[sample(n),]

# Create k equally size folds
k <- 10
dat2$fold <- cut(seq(1,n),breaks=k,labels=FALSE)
# Take a peek -- note the fold column
head(dat2,20)

# plot
ggplot(dat2,aes(x=x,y=y,color=factor(fold))) + 
  geom_point(size=3)

# object to store results
lm1Results <- data.frame(rsq=rep(NA,k), 
                         mae=rep(NA,k),
                         rmse=rep(NA,k))

# loop through each fold
for(i in 1:k){
  testing <- dat2[dat2$fold==i,]
  training <- dat[dat2$fold!=i,]
  
  lm1 <- lm(y~x,data=training)
  testing$yhat <- predict(lm1,newdata=testing)
  lm1Results$rsq[i] <- cor(testing$yhat,testing$y)^2
  lm1Results$mae[i] <- mean(abs(testing$yhat - testing$y))
  lm1Results$rmse[i] <- sqrt(mean((testing$yhat - testing$y)^2))
}
# Here is the mean rsq for lm1 
mean(lm1Results$rsq)

# and the SE on that mean
sd(lm1Results$rsq)/sqrt(k)

##############################################
### Leave-one-out Cross-validation (LOOCV) ###
##############################################

loocvResults <- data.frame(y=rep(NA,n), yhat=rep(NA,n ))

# loop n times and leave one out each time
for(i in 1:n){
  testing <- dat[i,]
  training <- dat[-i,]
  
  lm1 <- lm(y~x,data=training)
  testing$yhat <- predict(lm1,newdata=testing)
  loocvResults$y[i] <- testing$y
  loocvResults$yhat[i] <- testing$yhat
}
#rsq2
cor(loocvResults$yhat,loocvResults$y)^2

#mae
mean(abs(loocvResults$yhat - loocvResults$y))

#####################################
### Caret and Regression Training ###
#####################################

library(caret)
## 10-fold CV with 10 repeats
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10)

glm1 <- train(y ~ x, data = dat, 
                 method = "glm", 
                 trControl = fitControl)
glm1

# Note that if you run this, you might be prompted 
# to install the `gam` library.
gam1 <- train(y ~ x, data = dat, 
             method = "gamSpline", 
             trControl = fitControl)
gam1

newDat <- data.frame(x=seq(-4,4,by=0.1))
newDat$yhat <- predict(gam1,newdata = newDat)

ggplot() + 
  geom_point(data = dat,aes(x=x,y=y),size=3,shape=21,fill="red",alpha=0.8) + 
  geom_line(data = newDat,aes(x=x,y=yhat),size=1.1)
```

```{r eval = F, include = F}
### Classmate help
# object to store results
remotes::install_github("allisonhorst/palmerpenguins")
library(palmerpenguins)
library(tidyverse)

glimpse(penguins)

penguins2 <- penguins
penguins2 <- penguins %>%
  select(-sex) %>%
  drop_na()

penguins3 <- penguins %>%
  drop_na()

n <- nrow(penguins2)
penguins2 <- penguins2[sample(n),]

k <- 10

lm1Results <- data.frame(rsq=rep(NA,k), 
                         mae=rep(NA,k),
                         rmse=rep(NA,k))

penguins2$fold <- cut(seq(1,n),breaks=k,labels=FALSE)

# loop through each fold
for(i in 1:k){
  testing <- penguins2[penguins2$fold==i,]
  training <- penguins[penguins2$fold!=i,]

  lm1 <- lm(body_mass_g~bill_length_mm,data=training)
  testing$yhat <- predict(lm1,newdata=testing)
  lm1Results$rsq[i] <- cor(testing$yhat,testing$body_mass_g)^2
  lm1Results$mae[i] <- mean(abs(testing$yhat - testing$body_mass_g))
  lm1Results$rmse[i] <- sqrt(mean((testing$yhat - testing$body_mass_g)^2))
}
# Here is the mean rsq for lm1 
mean(lm1Results$rsq)
```

<!--- My Work Section --->

# Linear Model of Palmer Penguin Body Mass
```{r include = F}
ggplot(data=penguins,mapping = aes(x=bill_length_mm,y=body_mass_g,color=species)) +
  geom_point() + geom_smooth(method="lm") +
  labs(x="Bill length (mm)", y="Body mass (g)") +
  theme_classic()
```

I will be working with a simple linear model to assess penguin body mass as a function of bill length and species:
<br><br>
<center> $Body Mass (g)$ ~ $Bill Length + Species$ </center>
<br>

```{r class.source = 'fold-show'}
# Simple Linear Model

## Load Penguin Data
data(penguins)
penguins <- penguins %>%
  select(-sex) %>% # Drop 'sex' column to remove NAs without removing more data points
  drop_na() # Drop NA data points

## Linear Model
lmMass <- lm(body_mass_g ~ bill_length_mm + species , data = penguins)
```
<br>

# Validation of Linear Model

I will use repeated K-folds cross validation to validate my original linear model:

```{r class.source = 'fold-show', results = 'hide'}
# lm Penguin data: Body Mass ~ Bill Length + Species

## Repeat K-fold CV with caret
fitControl.rcv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10)

# Train lm with default parameters
lm.pengu.rcv <- train(body_mass_g ~ bill_length_mm + species, data = penguins,
                      method = "lm",
                      trControl = fitControl.rcv)

Pengu.predict <- penguins
Pengu.predict$yhat.rcv <- predict(lm.pengu.rcv, newdata = Pengu.predict) # new column in penguins with predictions
```
<br>

Figure 1 shows that the linear model appears to capture the general trend in the data points with high variance on both extreme ends. 

```{r fig.cap = '*Points are observed penguin body mass by Bill length. Blue line represents the predicted body mass from linear model.'}
# Plotting Predicted vs Observed
ggplot(data = Pengu.predict) +
  geom_smooth(aes(x = bill_length_mm, y = yhat.rcv)) +
  geom_point(aes(x = bill_length_mm, y = body_mass_g, color = species)) +
  theme_classic() +
  labs(title = 'Figure 1. Predicted vs Observed Penguin Body Mass') +
  xlab('Bill Length (mm)') +
  ylab('Predicted Body Mass (g)')
```

<br>
I plotted the residual $Pred\ \hat{y} - Obs\ y$ body mass values and my residuals appear to indicate that my linear model may be underpredicting (Figure 2). If we ignore the extreme ends of the data where the most variance exists, our smooth line of the residuals stays close to the $y=0$ line. I can expect predictions made at the lower and upper ends to be less accurate than between each end.

```{r fig.cap = '*Points are the linear model residuals of penguin body mass by bill length. Blue line represents the mean residual value'}
# Plotting Residuals
ggplot(data = Pengu.predict) +
  geom_smooth(aes(x = yhat.rcv, y = yhat.rcv - body_mass_g)) +
  geom_point(aes(x = yhat.rcv, y = yhat.rcv - body_mass_g, color = species)) +
  theme_classic() +
  labs(title = 'Figure 2.  LM Residuals of Penguin Body Mass') +
  xlab('Bill Length (mm)') +
  ylab('Predicted - Observed Body Mass (g)')
```
<br>

# Results Table

My repeated k-fold cross validated linear model performed nearly the same as my unvalidated model. My final results are shown in Table 1.

```{r}
# Report mean R^2 and MAE.
lmMass.sum <- summary(lmMass) # Contains r sq value
lmMass.MAE <- mean(abs(lmMass$residuals)) # Calculate lm MAE since not already given
meanRsq.rcv <- mean(lm.pengu.rcv$resample$Rsquared)
meanMAE.rcv <- mean(lm.pengu.rcv$resample$MAE)

# Build Table
penguSummary <- tibble(Model = c('lm', 'lm RCV'),
                      'R^2' = round(c(lmMass.sum$r.squared, meanRsq.rcv), 3),
                      MAE = round(c(lmMass.MAE, meanMAE.rcv), 0))

# Style Table
pengu.crossvaltable <- knitr::kable(penguSummary,
                                    format = "html", table.attr = "style='width:30%;'",
                                    align='rcc',
                                    caption = 'Table 1. Cross Validation Summary') %>%
  kableExtra::kable_styling(position = "center")

pengu.crossvaltable
```
<br><br>

